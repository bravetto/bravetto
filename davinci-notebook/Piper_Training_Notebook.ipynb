{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK3nmYDB6C1a"
      },
      "source": [
        "# <font color=\"ffc800\"> **[Piper](https://github.com/rhasspy/piper) training notebook.**\n",
        "## ![Piper logo](https://contribute.rhasspy.org/img/logo.png)\n",
        "\n",
        "---\n",
        "\n",
        "- Notebook made by [rmcpantoja](http://github.com/rmcpantoja)\n",
        "- Collaborator: [Xx_Nessu_xX](http://github.com/Xx_Nessu_xX)\n",
        "- With some modifications by [KiON-GiON](https://github.com/KiON-GiON)\n",
        "\n",
        "---\n",
        "\n",
        "# Notes:\n",
        "\n",
        "- <font color=\"orange\">**Things in orange mean that they are important.**\n",
        "\n",
        "# Credits:\n",
        "\n",
        "* [Feanix-Fyre fork](https://github.com/Feanix-Fyre/piper) with some improvements.\n",
        "* [Tacotron2 NVIDIA training notebook](https://github.com/justinjohn0306/FakeYou-Tacotron2-Notebook) - Dataset duration snippet.\n",
        "* [üê∏TTS](https://github.com/coqui-ai/TTS) - Resampler and XTTS formater demo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AICh6p5OJybj"
      },
      "source": [
        "# <font color=\"ffc800\">üîß ***First steps.*** üîß"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyxSMuzjfQrz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ## <font color=\"ffc800\"> **Google Colab Anti-Disconnect.** üîå\n",
        "#@markdown ---\n",
        "#@markdown #### Avoid automatic disconnection. Still, it will disconnect after <font color=\"orange\">**6 to 12 hours**</font>.\n",
        "\n",
        "import IPython\n",
        "js_code = '''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''\n",
        "display(IPython.display.Javascript(js_code))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygxzp-xHTC7T",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ## <font color=\"ffc800\"> **Check GPU type.** üëÅÔ∏è\n",
        "#@markdown ---\n",
        "#@markdown #### A higher capable GPU can lead to faster training speeds. By default, you will have a <font color=\"orange\">**Tesla T4**</font>.\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUNjId07JfAK",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # <font color=\"ffc800\"> **Mount Google Drive.** üìÇ\n",
        "#@markdown ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XwmTVlcUgCh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # <font color=\"ffc800\"> **Install software.** üì¶\n",
        "#@markdown ---\n",
        "#@markdown ####In this cell the synthesizer and its necessary dependencies to execute the training will be installed. (this may take a while)\n",
        "import os\n",
        "!apt-get -q update -y\n",
        "!apt-get -q install build-essential cmake ninja-build espeak-ng aria2\n",
        "\n",
        "# clone:\n",
        "REPO_URL = \"https://github.com/KiON-GiON/piper1-gpl.git\"\n",
        "REPO_DIR = \"/content/piper1-gpl\"\n",
        "\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    !git clone -q -b fixes {REPO_URL} {REPO_DIR}\n",
        "    %cd {REPO_DIR}\n",
        "!wget -q \"https://raw.githubusercontent.com/coqui-ai/TTS/dev/TTS/bin/resample.py\"\n",
        "!python -m pip install -e .[train]\n",
        "!bash build_monotonic_align.sh\n",
        "!pip install -q --upgrade gdown scikit-build protobuf==3.20.3\n",
        "!python setup.py build_ext --inplace\n",
        "!pip install -q faster-whisper\n",
        "# Useful vars:\n",
        "use_whisper = True\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3bMzEE0V5Ma"
      },
      "source": [
        "# <font color=\"ffc800\"> ü§ñ ***Training.*** ü§ñ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvEGjf0aV8eg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # <font color=\"ffc800\"> **1. Extract dataset.** üì•\n",
        "#@markdown ---\n",
        "#@markdown ####Important: the audios must be in <font color=\"orange\">**wav format, (16000 or 22050hz, 16-bits, mono), and, for convenience, numbered. Example:**\n",
        "\n",
        "#@markdown * <font color=\"orange\">**1.wav**</font>\n",
        "#@markdown * <font color=\"orange\">**2.wav**</font>\n",
        "#@markdown * <font color=\"orange\">**3.wav**</font>\n",
        "#@markdown * <font color=\"orange\">**.....**</font>\n",
        "\n",
        "#@markdown ---\n",
        "import os\n",
        "import wave\n",
        "import zipfile\n",
        "import datetime\n",
        "\n",
        "def get_dataset_duration(wav_path):\n",
        "    totalduration = 0\n",
        "    for file_name in [x for x in os.listdir(wav_path) if os.path.isfile(x) and \".wav\" in x]:\n",
        "        with wave.open(file_name, \"rb\") as wave_file:\n",
        "            frames = wave_file.getnframes()\n",
        "            rate = wave_file.getframerate()\n",
        "            duration = frames / float(rate)\n",
        "            totalduration += duration\n",
        "    wav_count = len(os.listdir(wav_path))\n",
        "    duration_str = str(datetime.timedelta(seconds=round(totalduration, 0)))\n",
        "    return wav_count, duration_str\n",
        "\n",
        "%cd /content\n",
        "if not os.path.exists(\"/content/dataset\"):\n",
        "    os.makedirs(\"/content/dataset\")\n",
        "    os.makedirs(\"/content/dataset/wavs\")\n",
        "%cd /content/dataset\n",
        "#@markdown ### Audio dataset path to unzip:\n",
        "zip_path = \"/content/drive/MyDrive/wavs.zip\" #@param {type:\"string\"}\n",
        "zip_path = zip_path.strip()\n",
        "if zip_path:\n",
        "    if os.path.exists(zip_path):\n",
        "        if zipfile.is_zipfile(zip_path):\n",
        "            print(\"Unzipping audio content...\")\n",
        "            !unzip -q -j \"{zip_path}\" -d /content/dataset/wavs\n",
        "        else:\n",
        "            print(\"Copying audio contents of this folder...\")\n",
        "            fp = zip_path + \"/.\"\n",
        "            !cp -a \"$fp\" \"/content/dataset/wavs\"\n",
        "    else:\n",
        "        raise Exception(\"The path provided to the wavs is not correct. Please set a valid path.\")\n",
        "else:\n",
        "    raise Exception(\"You must provide with a path to the wavs.\")\n",
        "if os.path.exists(\"/content/dataset/wavs/wavs\"):\n",
        "    for file in os.listdir(\"/content/dataset/wavs/wavs\"):\n",
        "        !mv /content/dataset/wavs/wavs/\"$file\"  /content/dataset/wavs/\"$file\"\n",
        "    !rm -r /content/dataset/wavs/*.txt\n",
        "    !rm -r /content/dataset/wavs/*.csv\n",
        "%cd /content/dataset/wavs\n",
        "audio_count, dataset_dur = get_dataset_duration(\"/content/dataset/wavs\")\n",
        "print(f\"Opened dataset with {audio_count} wavs with duration {dataset_dur}.\")\n",
        "%cd /content\n",
        "#@markdown ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0W0OCvXXvue",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # <font color=\"ffc800\"> **2. Upload the transcript file.** üìù\n",
        "#@markdown ---\n",
        "#@markdown ####<font color=\"orange\">**Important: the transcription means writing what the character says in each of the audios, and it must have the following structure:**\n",
        "\n",
        "#@markdown ##### <font color=\"orange\">For a single-speaker dataset:\n",
        "#@markdown * 1.wav|This is what my character says in audio 1.\n",
        "#@markdown * 2.wav|This, the text that the character says in audio 2.\n",
        "#@markdown * ...\n",
        "\n",
        "#@markdown ##### <font color=\"orange\">For a multi-speaker dataset:\n",
        "\n",
        "#@markdown * speaker1audio1.wav|speaker1|This is what the first speaker says.\n",
        "#@markdown * speaker1audio2.wav|speaker1|This is another audio of the first speaker.\n",
        "#@markdown * speaker2audio1.wav|speaker2|This is what the second speaker says in the first audio.\n",
        "#@markdown * speaker2audio2.wav|speaker2|This is another audio of the second speaker.\n",
        "#@markdown * ...\n",
        "\n",
        "#@markdown And so on. In addition, the transcript must be in a <font color=\"orange\">**.csv or .txt format. (UTF-8 without BOM)**\n",
        "\n",
        "#@markdown ## Auto-transcribe with whisper if transcription is not provided.\n",
        "\n",
        "#@markdown **Note: If you don't upload any transcription files, the wavs will be transcribed using the whisper tool when you execute the next step. Then, the notebook will continue with the rest of the preprocessing if there are no errors. Although the Whisper tool has good transcription results, in my experience I recommend transcribing manually and uploading it from this cell, since a good TTS voice needs to be optimized to give even better results. For example, when transcribing manually you will be able to observe every detail that the speaker makes (such as punctuation, sounds, etc.), and capture them in the transcription according to the speaker's intonations.**\n",
        "\n",
        "\n",
        "#@markdown However, if you want to transcribe and review this transcription, you can use the individual notebooks:\n",
        "\n",
        "#@markdown * [English](http://colab.research.google.com/github/rmcpantoja/My-Colab-Notebooks/blob/main/notebooks/OpenAI_Whisper_-_DotCSV_(Speech_dataset_multi-transcryption_support)en.ipynb)\n",
        "#@markdown * [French](http://colab.research.google.com/github/rmcpantoja/My-Colab-Notebooks/blob/main/notebooks/OpenAI_Whisper_-_DotCSV_(Speech_dataset_multi-transcryption_support)fr.ipynb)\n",
        "#@markdown * [Spanish](http://colab.research.google.com/github/rmcpantoja/My-Colab-Notebooks/blob/main/notebooks/OpenAI_Whisper_-_DotCSV_(Speech_dataset_multi-transcryption_support)es.ipynb)\n",
        "\n",
        "#@markdown ---\n",
        "%cd /content/dataset\n",
        "from google.colab import files\n",
        "!rm /content/dataset/metadata.csv\n",
        "\n",
        "if os.path.exists(\"/content/dataset/wavs/_transcription.txt\"):\n",
        "  !mv \"/content/dataset/wavs/_transcription.txt\" metadata.csv\n",
        "else:\n",
        "  listfn, length = files.upload().popitem()\n",
        "  if listfn != \"metadata.csv\":\n",
        "    !mv \"$listfn\" metadata.csv\n",
        "\n",
        "use_whisper = False\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOyx9Y6JYvRF",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # <font color=\"ffc800\"> **3. Preprocess dataset (metadata & setup).** üîÑ\n",
        "#@markdown ---\n",
        "import os\n",
        "if use_whisper:\n",
        "    import torch\n",
        "    from faster_whisper import WhisperModel\n",
        "    from tqdm import tqdm\n",
        "    from google import colab\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    def make_dataset(path, language):\n",
        "        metadata = \"\"\n",
        "        text = \"\"\n",
        "        files = [f for f in os.listdir(path) if f.endswith(\".wav\")]\n",
        "        assert len(files) > 0, \"You don't have wavs uploaded either! Please upload at least one zip with the wavs in step 2.\"\n",
        "        metadata_file = open(f\"{path}/../metadata.csv\", \"w\")\n",
        "        whisper = WhisperModel(\"large-v3\", device=device, compute_type=\"float16\")\n",
        "        for audio_file in tqdm(files):\n",
        "            full_path = os.path.join(path, audio_file)\n",
        "            segments, _ = whisper.transcribe(full_path, word_timestamps=False, language=language)\n",
        "            for segment in segments:\n",
        "                text += segment.text\n",
        "            text = text.strip()\n",
        "            text = text.replace('\\n', ' ')\n",
        "            metadata = f\"{audio_file}|{text}\\n\"\n",
        "            metadata_file.write(metadata)\n",
        "            text = \"\"\n",
        "        colab.files.download(f\"{path}/../metadata.csv\")\n",
        "        del whisper\n",
        "        return True\n",
        "\n",
        "#@markdown ### First of all, select the language of your dataset.\n",
        "language = \"English (U.S.)\" #@param [\"ÿ£ŸÑÿπŸéÿ±Ÿéÿ®ŸêŸä\", \"Catal√†\", \"ƒçe≈°tina\", \"Dansk\", \"Deutsch\", \"ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨\", \"English (British)\", \"English (U.S.)\", \"Espa√±ol (Castellano)\", \"Espa√±ol (Latinoamericano)\", \"Suomi\", \"Fran√ßais\", \"Magyar\", \"Icelandic\", \"Italiano\", \"·É•·Éê·É†·Éó·É£·Éö·Éò\", \"“õ–∞–∑–∞“õ—à–∞\", \"L√´tzebuergesch\", \"‡§®‡•á‡§™‡§æ‡§≤‡•Ä\", \"Nederlands\", \"Norsk\", \"Polski\", \"Portugu√™s (Brasil)\", \"Portugu√™s (Portugal)\", \"Rom√¢nƒÉ\", \"–†—É—Å—Å–∫–∏–π\", \"–°—Ä–ø—Å–∫–∏\", \"Svenska\", \"Kiswahili\", \"T√ºrk√ße\", \"—É–∫—Ä–∞—óÃÅ–Ω—Å—å–∫–∞\", \"Ti·∫øng Vi·ªát\", \"ÁÆÄ‰Ωì‰∏≠Êñá\"]\n",
        "#@markdown ---\n",
        "# language definition:\n",
        "languages = {\n",
        "    \"ÿ£ŸÑÿπŸéÿ±Ÿéÿ®ŸêŸä\": \"ar\",\n",
        "    \"Catal√†\": \"ca\",\n",
        "    \"ƒçe≈°tina\": \"cs\",\n",
        "    \"Dansk\": \"da\",\n",
        "    \"Deutsch\": \"de\",\n",
        "    \"ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨\": \"el\",\n",
        "    \"English (British)\": \"en\",\n",
        "    \"English (U.S.)\": \"en-us\",\n",
        "    \"Espa√±ol (Castellano)\": \"es\",\n",
        "    \"Espa√±ol (Latinoamericano)\": \"es-419\",\n",
        "    \"Suomi.\": \"fi\",\n",
        "    \"Fran√ßais\": \"fr\",\n",
        "    \"Magyar\": \"hu\",\n",
        "    \"Icelandic\": \"is\",\n",
        "    \"Italiano\": \"it\",\n",
        "    \"·É•·Éê·É†·Éó·É£·Éö·Éò\": \"ka\",\n",
        "    \"“õ–∞–∑–∞“õ—à–∞\": \"kk\",\n",
        "    \"L√´tzebuergesch\": \"lb\",\n",
        "    \"‡§®‡•á‡§™‡§æ‡§≤‡•Ä\": \"ne\",\n",
        "    \"Nederlands\": \"nl\",\n",
        "    \"Norsk\": \"nb\",\n",
        "    \"Polski\": \"pl\",\n",
        "    \"Portugu√™s (Brasil)\": \"pt-br\",\n",
        "    \"Portugu√™s (Portugal)\": \"pt-pt\",\n",
        "    \"Rom√¢nƒÉ\": \"ro\",\n",
        "    \"–†—É—Å—Å–∫–∏–π\": \"ru\",\n",
        "    \"–°—Ä–ø—Å–∫–∏\": \"sr\",\n",
        "    \"Svenska\": \"sv\",\n",
        "    \"Kiswahili\": \"sw\",\n",
        "    \"T√ºrk√ße\": \"tr\",\n",
        "    \"—É–∫—Ä–∞—óÃÅ–Ω—Å—å–∫–∞\": \"uk\",\n",
        "    \"Ti·∫øng Vi·ªát\": \"vi\",\n",
        "    \"ÁÆÄ‰Ωì‰∏≠Êñá\": \"zh\"\n",
        "}\n",
        "\n",
        "def _get_language(code):\n",
        "    return languages[code]\n",
        "\n",
        "final_language = _get_language(language)\n",
        "#@markdown ### Choose a name for your model:\n",
        "model_name = \"\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "# output:\n",
        "#@markdown ### Choose the working folder: (recommended to save to Drive)\n",
        "\n",
        "#@markdown The working folder will be used in preprocessing, but also in training the model.\n",
        "output_path = \"/content/drive/MyDrive/colab/piper\" #@param {type:\"string\"}\n",
        "output_dir = output_path+\"/\"+model_name\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "#@markdown ### Select the sample rate of the dataset:\n",
        "sample_rate = \"22050\" #@param [\"16000\", \"22050\"]\n",
        "#@markdown ---\n",
        "# creating paths:\n",
        "audio_cache_dir = \"/content/audio_cache\"\n",
        "os.makedirs(audio_cache_dir, exist_ok=True)\n",
        "#@markdown ### Do you want to train using this sample rate, but your audios don't have it?\n",
        "#@markdown The resampler helps you do it quickly!\n",
        "resample = False #@param {type:\"boolean\"}\n",
        "\n",
        "%cd {REPO_DIR}\n",
        "\n",
        "if resample:\n",
        "  !python resample.py --input_dir \"/content/dataset/wavs\" --output_dir \"/content/dataset/wavs_resampled\" --output_sr {sample_rate} --file_ext \"wav\"\n",
        "  !mv /content/dataset/wavs_resampled/* /content/dataset/wavs\n",
        "#@markdown ---\n",
        "# check transcription:\n",
        "if use_whisper:\n",
        "    print(\"Transcript file hasn't been uploaded. Transcribing these audios using Whisper...\")\n",
        "    make_dataset(\"/content/dataset/wavs\", final_language[:2])\n",
        "    print(\"Transcription done! Metadata ready!\")\n",
        "print(\"Metadata and basic settings ready. Actual preprocessing will be done during training.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ickQlOCRjkBL",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # <font color=\"ffc800\"> **4. Settings.** üß∞\n",
        "#@markdown ---\n",
        "import json\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from google.colab import output\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import csv\n",
        "\n",
        "metadata_path = \"/content/dataset/metadata.csv\"\n",
        "detected_num_speakers = 1\n",
        "\n",
        "if os.path.exists(metadata_path):\n",
        "    speakers = set()\n",
        "    with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f, delimiter=\"|\")\n",
        "        for row in reader:\n",
        "            if len(row) >= 3:\n",
        "                speakers.add(row[1].strip())\n",
        "    if speakers:\n",
        "        detected_num_speakers = len(speakers)\n",
        "\n",
        "print(f\"Detected {detected_num_speakers} speaker(s) from metadata.csv.\")\n",
        "\n",
        "#@markdown ### Override detected number of speakers (0 = use detected)\n",
        "override_num_speakers = 0 #@param {type:\"integer\"}\n",
        "if override_num_speakers > 0:\n",
        "    model_num_speakers = override_num_speakers\n",
        "else:\n",
        "    model_num_speakers = detected_num_speakers\n",
        "\n",
        "if model_num_speakers > 1:\n",
        "    num_speakers_arg = f\"--model.num_speakers {model_num_speakers} \"\n",
        "    print(f\"Using multi-speaker model with {model_num_speakers} speakers.\")\n",
        "else:\n",
        "    num_speakers_arg = \"\"\n",
        "    print(\"Using single-speaker model.\")\n",
        "#@markdown ### <font color=\"orange\">**Select the action to train this dataset: (READ CAREFULLY)**</font>\n",
        "\n",
        "#@markdown * The option to <font color=\"orange\">continue a training</font> is self-explanatory. If you've previously trained a model with free colab, your time is up and you're considering training it some more, this is ideal for you. You just have to set the same settings that you set when you first trained this model.\n",
        "#@markdown * The option to <font color=\"orange\">convert a single-speaker model to a multi-speaker model</font> is self-explanatory, and for this it is important that you have processed a dataset that contains text and audio from all possible speakers that you want to train in your model.\n",
        "#@markdown * The <font color=\"orange\">finetune</font> option is used to train a dataset using a pretrained model, that is, train on that data. This option is ideal if you want to train a very small dataset (more than five minutes recommended).\n",
        "#@markdown * The <font color=\"orange\">train from scratch</font> option builds features such as dictionary and speech form from scratch, and this may take longer to converge. For this, hours of audio (8 at least) are recommended, which have a large collection of phonemes.\n",
        "\n",
        "action = \"finetune\" #@param [\"Continue training\", \"convert single-speaker to multi-speaker model\", \"finetune\", \"train from scratch\"]\n",
        "#@markdown ---\n",
        "train_resume_arg = \"\"\n",
        "init_from_arg = \"\"\n",
        "\n",
        "#@markdown ### Use external checkpoint instead of the default pretrained models selection?\n",
        "use_external_checkpoint = False  #@param {type:\"boolean\"}\n",
        "#@markdown ### Path to external checkpoint (if enabled above)\n",
        "#@markdown Accepts:\n",
        "#@markdown - Google drive links.\n",
        "#@markdown - HTTP/HTTPS links.\n",
        "#@markdown - Mounted Google Drive path (i.e. **/content/drive/MyDrive/pretrained.ckpt**).\n",
        "external_checkpoint_path = \"\"  #@param {type:\"string\"}\n",
        "if action == \"convert single-speaker to multi-speaker model\" and model_num_speakers <= 1:\n",
        "    raise Exception(\"You selected 'convert single-speaker to multi-speaker model' but metadata.csv only shows 1 speaker. Please provide a multi-speaker metadata or adjust override_num_speakers.\")\n",
        "\n",
        "if action == \"Continue training\":\n",
        "    pattern = os.path.join(output_dir, \"lightning_logs\", \"**\", \"checkpoints\", \"last.ckpt\")\n",
        "    checkpoints = glob.glob(pattern, recursive=True)\n",
        "    if len(checkpoints):\n",
        "        def _version_num(path):\n",
        "            m = re.search(r'version_(\\d+)', path)\n",
        "            return int(m.group(1)) if m else -1\n",
        "\n",
        "        last_checkpoint = sorted(checkpoints, key=_version_num)[-1]\n",
        "        train_resume_arg = f'--ckpt_path \"{last_checkpoint}\" '\n",
        "        print(f\"Continuing {model_name}'s training from: {last_checkpoint}\")\n",
        "    else:\n",
        "        raise Exception(\"Training cannot be continued as there is no checkpoint to continue at.\")\n",
        "\n",
        "elif action in (\"finetune\", \"convert single-speaker to multi-speaker model\"):\n",
        "    if os.path.exists(os.path.join(output_dir, \"lightning_logs\", \"version_0\", \"checkpoints\", \"last.ckpt\")):\n",
        "        raise Exception(\"Oh no! You have already trained this model before, you cannot choose this option since your progress will be lost. Please select the option to continue a training.\")\n",
        "    else:\n",
        "        if use_external_checkpoint:\n",
        "            import re\n",
        "\n",
        "            ext_ckpt = external_checkpoint_path.strip()\n",
        "            if not ext_ckpt:\n",
        "                raise Exception(\"You enabled 'use external checkpoint' but did not provide a path or URL.\")\n",
        "\n",
        "            ckpt_local = ext_ckpt\n",
        "\n",
        "            is_url = ext_ckpt.startswith(\"http://\") or ext_ckpt.startswith(\"https://\")\n",
        "            is_drive_url = \"drive.google.com\" in ext_ckpt\n",
        "            is_drive_id = (not is_url) and ext_ckpt.startswith(\"1\") and (\"/\" not in ext_ckpt)\n",
        "\n",
        "            if is_url or is_drive_id:\n",
        "                print(\"\\033[93mDownloading external checkpoint...\")\n",
        "\n",
        "                if is_drive_url or is_drive_id:\n",
        "                    if is_drive_url:\n",
        "                        !gdown -q \"{ext_ckpt}\" -O \"/content/pretrained.ckpt\" --fuzzy\n",
        "                    else:\n",
        "                        !gdown -q \"{ext_ckpt}\" -O \"/content/pretrained.ckpt\"\n",
        "                else:\n",
        "                    !aria2c --quiet=true --file-allocation=none -d \"/content\" -o \"pretrained.ckpt\" \"{ext_ckpt}\"\n",
        "\n",
        "                ckpt_local = \"/content/pretrained.ckpt\"\n",
        "\n",
        "                if not os.path.exists(ckpt_local):\n",
        "                    raise Exception(\"Couldn't download the external checkpoint!\")\n",
        "                else:\n",
        "                    print(\"\\033[93mExternal checkpoint downloaded!\")\n",
        "            else:\n",
        "                if not os.path.exists(ext_ckpt):\n",
        "                    print(f\"Warning: external checkpoint path does not exist yet: {ext_ckpt}\")\n",
        "                    print(\"Make sure to upload/mount it before running the training cell.\")\n",
        "\n",
        "            init_from_arg = f'--model.init_from_checkpoint \"{ckpt_local}\" '\n",
        "            print(f\"Using external checkpoint for init_from_checkpoint: {ckpt_local}\")\n",
        "\n",
        "        else:\n",
        "            pretrained_json = os.path.join(REPO_DIR, \"notebooks\", \"pretrained_models.json\")\n",
        "            if not os.path.exists(pretrained_json):\n",
        "                print(\"Warning: pretrained_models.json not found. You must provide your own checkpoint at /content/pretrained.ckpt (or enable 'use external checkpoint').\")\n",
        "                init_from_arg = '--model.init_from_checkpoint \"/content/pretrained.ckpt\" '\n",
        "            else:\n",
        "                try:\n",
        "                    with open(pretrained_json, \"r\", encoding=\"utf-8\") as f:\n",
        "                        pretrained_models = json.load(f)\n",
        "\n",
        "                    if final_language in pretrained_models:\n",
        "                        models = pretrained_models[final_language]\n",
        "                        model_options = [(model_name, model_name) for model_name, model_url in models.items()]\n",
        "                        model_dropdown = widgets.Dropdown(description = \"Choose pretrained model\", options=model_options)\n",
        "                        download_button = widgets.Button(description=\"Download\")\n",
        "\n",
        "                        def download_model(btn):\n",
        "                            model_name_sel = model_dropdown.value\n",
        "                            model_url = pretrained_models[final_language][model_name_sel]\n",
        "                            print(\"\\033[93mDownloading pretrained model...\")\n",
        "                            if model_url.startswith(\"1\"):\n",
        "                                !gdown -q \"{model_url}\" -O \"/content/pretrained.ckpt\"\n",
        "                            elif model_url.startswith(\"https://drive.google.com/file/d/\"):\n",
        "                                !gdown -q \"{model_url}\" -O \"/content/pretrained.ckpt\" --fuzzy\n",
        "                            else:\n",
        "                                !aria2c --quiet=true --file-allocation=none -d \"/content\" -o \"pretrained.ckpt\" \"{model_url}\"\n",
        "                            model_dropdown.close()\n",
        "                            download_button.close()\n",
        "                            output.clear()\n",
        "                            if os.path.exists(\"/content/pretrained.ckpt\"):\n",
        "                                print(\"\\033[93mModel downloaded!\")\n",
        "                            else:\n",
        "                                raise Exception(\"Couldn't download the pretrained model!\")\n",
        "\n",
        "                        download_button.on_click(download_model)\n",
        "                        display(model_dropdown, download_button)\n",
        "                    else:\n",
        "                        raise Exception(f\"There are no pretrained models available for the language {final_language}\")\n",
        "                except FileNotFoundError:\n",
        "                    raise Exception(\"The pretrained_models.json file was not found.\")\n",
        "\n",
        "                init_from_arg = '--model.init_from_checkpoint \"/content/pretrained.ckpt\" '\n",
        "\n",
        "else:\n",
        "    print(\"\\033[93mWarning: this model will be trained from scratch. You need at least 8 hours of data for everything to work decent. Good luck!\")\n",
        "\n",
        "#@markdown ### Choose batch size based on this dataset:\n",
        "batch_size = 12 #@param {type:\"integer\"}\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ### Choose the quality for this model:\n",
        "#@markdown * x-low - 16Khz audio, 5-7M params\n",
        "#@markdown * medium - 22.05Khz audio, 15-20 params\n",
        "#@markdown * high - 22.05Khz audio, 28-32M params\n",
        "quality = \"medium\" #@param [\"high\", \"x-low\", \"medium\"]\n",
        "if quality == \"x-low\":\n",
        "    quality_args_cli = (\n",
        "        \"--model.hidden_channels 96 \"\n",
        "        \"--model.inter_channels 96 \"\n",
        "        \"--model.filter_channels 384 \"\n",
        "    )\n",
        "elif quality == \"high\":\n",
        "    quality_args_cli = (\n",
        "        \"--model.resblock 1 \"\n",
        "        '--model.resblock_kernel_sizes \"(3, 7, 11)\" '\n",
        "        '--model.resblock_dilation_sizes \"((1, 3, 5), (1, 3, 5), (1, 3, 5))\" '\n",
        "        '--model.upsample_rates \"(8, 8, 2, 2)\" '\n",
        "        \"--model.upsample_initial_channel 512 \"\n",
        "        '--model.upsample_kernel_sizes \"(16, 16, 4, 4)\" '\n",
        "    )\n",
        "else:\n",
        "    quality_args_cli = \"\"\n",
        "#@markdown ---\n",
        "#@markdown ### Checkpoint settings (how your training is saved)\n",
        "\n",
        "#@markdown **What is a checkpoint?**\n",
        "#@markdown A *checkpoint* is a snapshot of the model during training.\n",
        "#@markdown It lets you stop training and continue later, or go back to an earlier version.\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### **`checkpoint_epochs` ‚Äì How often to save checkpoints (in epochs)**\n",
        "#@markdown - If **> 0**, this value is used as an interval (in epochs):\n",
        "#@markdown   - to update `last.ckpt` (the most recent state), if `save_last` is enabled,\n",
        "#@markdown   - and for Lightning‚Äôs internal `ModelCheckpoint` callback (controlled by `num_ckpt`) to decide when it is allowed to write a checkpoint.\n",
        "#@markdown - If **0**, the interval is disabled:\n",
        "#@markdown   - `last.ckpt` is only written once at the very end of training (if `save_last` is enabled),\n",
        "#@markdown   - `ModelCheckpoint` falls back to Lightning‚Äôs default behavior (typically once per validation epoch when validation is enabled).\n",
        "checkpoint_epochs = 5  #@param {type:\"integer\"}\n",
        "#@markdown ---\n",
        "#@markdown ### **`save_last` ‚Äì Always keep a `last.ckpt` with the latest model**\n",
        "#@markdown - If **`True`**:\n",
        "#@markdown   - A custom callback (`LastCheckpoint`) writes **only** a file named **`last.ckpt`**.\n",
        "#@markdown   - This file is overwritten:\n",
        "#@markdown     - every `checkpoint_epochs` epochs (if `checkpoint_epochs > 0`),\n",
        "#@markdown     - and **always** one more time at the very end of training (provided Colab does not interrupt it).\n",
        "#@markdown   - It works with or without validation and **does not depend on any metric**.\n",
        "#@markdown - If **`False`**:\n",
        "#@markdown   - No `last.ckpt` is created; only the checkpoints managed by `num_ckpt` are saved (if any).\n",
        "save_last = False       #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown ### **`num_ckpt` ‚Äì How many checkpoints to keep from `ModelCheckpoint`**\n",
        "#@markdown This controls Lightning‚Äôs standard `ModelCheckpoint` callback.\n",
        "#@markdown\n",
        "#@markdown - **`num_ckpt > 0`**\n",
        "#@markdown   - With validation **enabled**:\n",
        "#@markdown     - Up to `num_ckpt` ‚Äúbest‚Äù checkpoints according to `val_loss` (mode: `min`) are kept.\n",
        "#@markdown     - The check to save is done every `checkpoint_epochs` epochs (if `checkpoint_epochs > 0`).\n",
        "#@markdown   - With validation **disabled**:\n",
        "#@markdown     - There is no validation metric, so this behaves like `0` (no ‚Äúbest‚Äù checkpoints are saved).\n",
        "#@markdown\n",
        "#@markdown - **`num_ckpt = 0`**\n",
        "#@markdown   - Disables metric‚Äëbased checkpoint saving.\n",
        "#@markdown   - With validation enabled, you still see the `val_loss` curve and audio samples,\n",
        "#@markdown     but **no** checkpoints are kept by `ModelCheckpoint`; only `last.ckpt` is written if `save_last` is `True`.\n",
        "#@markdown\n",
        "#@markdown - **`num_ckpt = -1`**\n",
        "#@markdown   - Keeps **all** checkpoints that are triggered by `ModelCheckpoint`.\n",
        "#@markdown   - With validation **enabled**:\n",
        "#@markdown     - A new checkpoint is written every `checkpoint_epochs` epochs (if `checkpoint_epochs > 0`) based on the validation cycle.\n",
        "#@markdown   - With validation **disabled**:\n",
        "#@markdown     - A new checkpoint is written every `checkpoint_epochs` epochs (if `checkpoint_epochs > 0`) without using any metric.\n",
        "#@markdown\n",
        "#@markdown In practice:\n",
        "#@markdown - Use a small positive `num_ckpt` (e.g. 1‚Äì3) if you only care about the best models by `val_loss`.\n",
        "#@markdown - Use **`num_ckpt = -1`** if you want to keep a snapshot at every checkpoint interval.\n",
        "#@markdown - Use **`num_ckpt = 0`** if you want to rely only on `last.ckpt` (plus any manual exports you do later).\n",
        "num_ckpt = 0           #@param {type:\"integer\"}\n",
        "#@ markdown ---\n",
        "#@markdown ### Step interval to generate model samples:\n",
        "log_every_n_steps = 1000 #@param {type:\"integer\"}\n",
        "#@markdown ---\n",
        "#@markdown ### Training epochs:\n",
        "max_epochs = 10000 #@param {type:\"integer\"}\n",
        "#@markdown ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpKDfhAHjHJ3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # <font color=\"ffc800\"> **5. Run the TensorBoard extension.** üìà\n",
        "#@markdown ---\n",
        "#@markdown The TensorBoard is used to visualize the results of the model while it's being trained such as audio and losses.\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {output_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4zbSjXg2J3N",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # <font color=\"ffc800\"> **6. Train.** üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
        "#@markdown ---\n",
        "#@markdown ### Run this cell to train your final model!\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### <font color=\"orange\">**Disable validation?**\n",
        "#@markdown By unchecking this checkbox, training will use the full dataset without\n",
        "#@markdown holding out any audio files for validation. No validation audio will be\n",
        "#@markdown generated in TensorBoard during training. This is only recommended for\n",
        "#@markdown extremely small datasets.\n",
        "validation = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown **Validation split (fraction of examples used for validation)**\n",
        "#@markdown Recommended values:\n",
        "#@markdown * Very small datasets (< 100 utterances): `0.0‚Äì0.02` (or disable validation).\n",
        "#@markdown * Small datasets (100‚Äì500 utterances): around `0.05`.\n",
        "#@markdown * Medium / large datasets (> 500 utterances): `0.05‚Äì0.10`.\n",
        "#@markdown \n",
        "#@markdown Try to keep **at least ~50 validation utterances**, but usually not more than **10% of the dataset**.\n",
        "validation_fraction = 0.01 #@param {type:\"number\"}\n",
        "\n",
        "if validation and validation_fraction > 0:\n",
        "    validation_args = (\n",
        "        f\"--data.validation_split {validation_fraction} \"\n",
        "        \"--data.num_test_examples 1 \"\n",
        "    )\n",
        "else:\n",
        "    validation_args = (\n",
        "        \"--data.validation_split 0 \"\n",
        "        \"--data.num_test_examples 0 \"\n",
        "    )\n",
        "\n",
        "import os\n",
        "\n",
        "config_filename = \"config.json\"\n",
        "config_path = os.path.join(output_dir, config_filename)\n",
        "\n",
        "\n",
        "cmd = (\n",
        "    f\"cd {REPO_DIR} && \"\n",
        "    \"python -m piper.train fit \"\n",
        "    f'--data.voice_name \"{model_name}\" '\n",
        "    f'--data.csv_path \"/content/dataset/metadata.csv\" '\n",
        "    f'--data.audio_dir \"/content/dataset/wavs\" '\n",
        "    f'--data.espeak_voice \"{final_language}\" '\n",
        "    f'--data.cache_dir \"{audio_cache_dir}\" '\n",
        "    f'--data.config_path \"{config_path}\" '\n",
        "    f\"--data.batch_size {batch_size} \"\n",
        "    f\"--model.sample_rate {sample_rate} \"\n",
        ")\n",
        "\n",
        "if model_num_speakers > 1:\n",
        "    cmd += f\"--model.num_speakers {model_num_speakers} \"\n",
        "\n",
        "cmd += quality_args_cli + \" \"\n",
        "\n",
        "cmd += (\n",
        "    f'--trainer.default_root_dir \"{output_dir}\" '\n",
        "    \"--trainer.accelerator gpu \"\n",
        "    \"--trainer.devices 1 \"\n",
        "    f\"--trainer.max_epochs {max_epochs} \"\n",
        "    f\"--trainer.log_every_n_steps {log_every_n_steps} \"\n",
        "    \"--trainer.precision 16-mixed \"\n",
        ")\n",
        "\n",
        "cmd += validation_args\n",
        "\n",
        "if num_ckpt == -1:\n",
        "    if checkpoint_epochs > 0:\n",
        "        cmd += f\"--checkpoint.every_n_epochs {checkpoint_epochs} \"\n",
        "    cmd += \"--checkpoint.save_top_k -1 \"\n",
        "\n",
        "    if validation:\n",
        "        cmd += \"--checkpoint.monitor val_loss \"\n",
        "        cmd += \"--checkpoint.mode min \"\n",
        "    else:\n",
        "        cmd += \"--checkpoint.monitor null \"\n",
        "\n",
        "elif validation and num_ckpt > 0:\n",
        "    if checkpoint_epochs > 0:\n",
        "        cmd += f\"--checkpoint.every_n_epochs {checkpoint_epochs} \"\n",
        "    cmd += f\"--checkpoint.save_top_k {num_ckpt} \"\n",
        "    cmd += \"--checkpoint.monitor val_loss \"\n",
        "    cmd += \"--checkpoint.mode min \"\n",
        "\n",
        "else:\n",
        "    cmd += \"--checkpoint.save_top_k 0 \"\n",
        "    cmd += \"--checkpoint.monitor null \"\n",
        "\n",
        "if save_last:\n",
        "    if checkpoint_epochs > 0:\n",
        "        cmd += f\"--last_checkpoint.every_n_epochs {checkpoint_epochs} \"\n",
        "    else:\n",
        "        cmd += \"--last_checkpoint.every_n_epochs null \"\n",
        "\n",
        "cmd += train_resume_arg\n",
        "cmd += init_from_arg\n",
        "\n",
        "print(\"Running command:\\n\", cmd)\n",
        "get_ipython().system(cmd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ISG085SYn85"
      },
      "source": [
        "#  <font color=\"orange\">**Have you finished training and want to test the model?**\n",
        "\n",
        "* If you want to run this model in any software that Piper integrates or the same Piper app, export your model using the [model exporter notebook](https://colab.research.google.com/github/KiON-GiON/piper1-gpl/blob/fixes/notebooks/Piper_ONNX_Export.ipynb)!\n",
        "* Wait! I want to test this right now before exporting it to the supported format for Piper. Test your generated last.ckpt with [this notebook](https://colab.research.google.com/github/rmcpantoja/piper/blob/master/notebooks/piper_inference_(ckpt).ipynb)! (**Outdated**)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}